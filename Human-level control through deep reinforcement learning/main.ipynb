{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "from PIL import Image\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gym.make('CartPole-v1', render_mode = 'rgb_array').unwrapped\n",
    "world.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self, world, screen_width, device):\n",
    "        self.resizer = T.Compose([\n",
    "            T.ToPILImage(),\n",
    "            T.Resize(40, interpolation=Image.BICUBIC),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        self.world = world\n",
    "        # self.world.reset()\n",
    "        self.screen_width = screen_width\n",
    "        self.device = device\n",
    "\n",
    "    def get_cart_location(self, world):\n",
    "        world_width = world.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        return int(world.state[0] * scale + self.screen_width / 2.0)\n",
    "    \n",
    "    def get_screen(self, world):\n",
    "        screen = world.render()[-1].transpose((2, 0, 1))\n",
    "        screen = screen[:, 160:320]\n",
    "        view_width = 320\n",
    "        cart_location = self.get_cart_location(world)\n",
    "\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (self.screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)\n",
    "\n",
    "        screen = screen[:, :, slice_range]\n",
    "        screen = torch.Tensor(screen) / 255\n",
    "        # screen = torch.from_numpy(screen)\n",
    "        return self.resizer(screen).unsqueeze(0).to(self.device).contiguous()\n",
    "\n",
    "environment = Environment(world, 600, device)\n",
    "screen = environment.get_screen(world)\n",
    "# screen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DQN, self).__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "\t\tself.bn1 = nn.BatchNorm2d(16)\n",
    "\t\tself.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(32)\n",
    "\t\tself.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(32)\n",
    "\t\tself.head = nn.Linear(448, 2)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.bn1(self.conv1(x)))\n",
    "\t\tx = F.relu(self.bn2(self.conv2(x)))\n",
    "\t\tx = F.relu(self.bn3(self.conv3(x)))\n",
    "\t\treturn self.head(x.view(x.size(0), -1))\n",
    "\t\n",
    "dqn = DQN().to(device)\n",
    "dqn(screen).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\tdef __init__(self, capacity):\n",
    "\t\t# self.capacity = capacity\n",
    "\t\tself.memory = deque(maxlen=capacity)\n",
    "\n",
    "\tdef push(self, *args):\n",
    "\t\tself.memory.append(Transition(*args))\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\treturn random.sample(self.memory, batch_size)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.memory)\n",
    "\t\n",
    "# memory = ReplayMemory(10000)\n",
    "# memory.push(1, 2, 3, 4)\n",
    "# memory.sample(5)\n",
    "# memory.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\tdef __init__(self, device):\n",
    "\t\tself.device = device\n",
    "\t\tself.policy_net = DQN().to(self.device)\n",
    "\t\tself.target_net = DQN().to(self.device)\n",
    "\t\tself.optimizer = optim.Adam(self.policy_net.parameters())\n",
    "\t\tself.criterion = nn.SmoothL1Loss()\n",
    "\t\tself.memory = ReplayMemory(100000)\n",
    "\t\tself.steps_done = 0\n",
    "\n",
    "\t\tself.EPSILON_END = 0.05\n",
    "\t\tself.EPSILON_START = 0.9\n",
    "\t\tself.EPSILON_DECAY = 500\n",
    "\t\tself.GAMMA = 0.99\n",
    "\n",
    "\t\tself.BATCH_SIZE = 128\n",
    "\n",
    "\tdef remember(self, *args):\n",
    "\t\tself.memory.push(*args)\n",
    "\n",
    "\tdef select_action(self, state):\n",
    "\t\t# Select an action according to an epsilon greedy approach\n",
    "\t\tsample = random.random()\n",
    "\t\tepsilon_threshold = self.EPSILON_END + (self.EPSILON_START - self.EPSILON_END) * math.exp(-1. * self.steps_done / self.EPSILON_DECAY) \n",
    "\t\tself.steps_done += 1\n",
    "\t\tif sample < epsilon_threshold:\n",
    "\t\t\treturn torch.tensor([[random.randrange(2)]], device=self.device, dtype=torch.long) # [1, 1]\n",
    "\t\telse:\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\treturn self.policy_net(state).max(1)[1].view(1, 1) # [1, 1]\n",
    "\n",
    "\tdef optimize_model(self):\n",
    "\t\tif len(self.memory) < self.BATCH_SIZE:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\ttransitions = self.memory.sample(self.BATCH_SIZE)\n",
    "\t\tbatch = Transition(*zip(*transitions))\n",
    "\t\n",
    "\t\tnon_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=self.device, dtype=torch.uint8)\n",
    "\t\tnon_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "\t\tstate_batch = torch.cat(batch.state)\n",
    "\t\taction_batch = torch.cat(batch.action)\n",
    "\t\treward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\t\tstate_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\t\t\n",
    "\t\tnext_state_values = torch.zeros(self.BATCH_SIZE, device=self.device)\n",
    "\t\tnext_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "\t\texpected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "\t\tloss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\tfor param in self.policy_net.parameters():\n",
    "\t\t\tparam.grad.data.clamp_(-1, 1)\n",
    "\t\tself.optimizer.step()\n",
    "\n",
    "agent = Agent(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "TARGET_UPDATE = 10\n",
    "episode_rewards = []\n",
    "plot_frequency = 10\n",
    "\n",
    "for i in range(500):\n",
    "\tworld.reset()\n",
    "\n",
    "\t# next_screen = environment.get_screen(world)\n",
    "\tcurrent_screen = environment.get_screen(world)\n",
    "\tepisode_reward = 0\n",
    "\n",
    "\tfor t in count():\n",
    "\t\taction = agent.select_action(current_screen)\n",
    "\t\t_, reward, terminated, truncated, _ = world.step(action.item())\n",
    "\t\tnext_screen = environment.get_screen(world)\n",
    "\t\tepisode_reward += reward\n",
    "\t\tdone = terminated or truncated\n",
    "\t\treward = torch.tensor([reward], device=device)\n",
    "\n",
    "\t\tagent.remember(current_screen, action, next_screen, reward)\n",
    "\t\tcurrent_screen = next_screen\n",
    "\n",
    "\t\tagent.optimize_model()\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tdurations.append(t + 1)\n",
    "\t\t\tbreak\n",
    "\t\n",
    "\tepisode_rewards.append(episode_reward)\n",
    "\tif i % TARGET_UPDATE == 0:\n",
    "\t\tagent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "\n",
    "\tif i % plot_frequency == 0:\n",
    "\t\t# break\n",
    "\t\tclear_output(wait=True)\n",
    "\t\tprint('Episode: ', i, 'Mean reward: ', np.mean(episode_rewards[-plot_frequency:]))\n",
    "\t\tplt.plot(episode_rewards)\n",
    "\t\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), 'model_weights.pth')\n",
    "# agent.policy_net.load_state_dict(torch.load('model_weights.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN().to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.x_threshold to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.x_threshold` for environment variables or `env.get_wrapper_attr('x_threshold')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\User\\miniconda3\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.state to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.state` for environment variables or `env.get_wrapper_attr('state')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready f:\\OUT\\Github\\Human-level control through deep reinforcement learning\\videos/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "_ = env.reset()\n",
    "step_starting_index = 0\n",
    "episode_index = 0\n",
    "for step_index in range(199): \n",
    "   # action = env.action_space.sample()\n",
    "   screen = environment.get_screen(env)\n",
    "   model_output = model(screen)\n",
    "   action = model_output.max(1)[1].item()\n",
    "   _, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      save_video(\n",
    "         env.render(),\n",
    "         \"videos\",\n",
    "         fps=env.metadata[\"render_fps\"],\n",
    "         step_starting_index=step_starting_index,\n",
    "         episode_index=episode_index\n",
    "      )\n",
    "      step_starting_index = step_index + 1\n",
    "      episode_index += 1\n",
    "      env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
