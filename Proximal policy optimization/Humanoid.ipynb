{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, MultivariateNormal\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Humanoid-v4\")\n",
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim, action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim=state_dim, action_dim=action_dim, has_continuous_action_space=True, action_std_init=0.6):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        \n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        if has_continuous_action_space :\n",
    "            self.actor_mouth = nn.Sequential(\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.actor_mouth = nn.Sequential(\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "            )\n",
    "\n",
    "        self.critic_mouth = nn.Sequential(\n",
    "                        nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.head(state)\n",
    "        action_mean_prob = self.actor_mouth(x)\n",
    "        state_val = self.critic_mouth(x)\n",
    "        return action_mean_prob, state_val\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean, state_val = self(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs, state_val = self(state)\n",
    "            dist = Categorical(action_probs)\n",
    "            \n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean, state_val = self(state)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "        else:\n",
    "            action_probs, state_val = self(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        return action_logprobs, state_val, dist_entropy\n",
    "\n",
    "\n",
    "\n",
    "ac = ActorCritic().to(device)\n",
    "inp = torch.rand(1, state_dim).to(device)\n",
    "action, action_logprob, state_val = ac.act(inp)\n",
    "ac.evaluate(inp, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim=state_dim, action_dim=action_dim, lr_actor=0.0003 , lr_critic=0.001 , gamma=0.99 , K_epochs=80, eps_clip=0.2, has_continuous_action_space=True, action_std_init=0.6):\n",
    "        \n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_actor)\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "        self.update_timestep = 10000\n",
    "        def create_discount_matrix(gamma, n):\n",
    "            temp = torch.tensor([gamma ** i for i in range(n)], dtype=torch.float32)\n",
    "            final = torch.zeros((n, n), dtype=torch.float32)\n",
    "            for i in range(n):\n",
    "                final[i:, i] = temp[:n-i]\n",
    "\n",
    "            return final\n",
    "\n",
    "        self.discount_matrix = create_discount_matrix(gamma, self.update_timestep).to(device)\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            if self.has_continuous_action_space:\n",
    "                return action.cpu().numpy().flatten()\n",
    "            else:\n",
    "                return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        rewards = torch.tensor(self.buffer.rewards).view(1, -1).float().to(device)\n",
    "        rewards = rewards @ self.discount_matrix\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        rewards = rewards.view(-1)\n",
    "\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "  \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "ppo_agent = PPO()\n",
    "ppo_agent.load(f\"ppo_actor.pt\")\n",
    "list_episodes_reward = []\n",
    "time_step = 0\n",
    "i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_len = 1000\n",
    "ppo_agent.update_timestep = max_ep_len * 4 \n",
    "ppo_agent.__init__()\n",
    "ppo_agent.load(f\"ppo_actor.pt\")\n",
    "max_training_timesteps = int(3e6) \n",
    "env = gym.make(\"Humanoid-v4\")\n",
    "\n",
    "\n",
    "action_std_decay_freq = int(2.5e5)\n",
    "print_freq = max_ep_len * 10\n",
    "save_model_freq = int(1e4)\n",
    "\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "\n",
    "\n",
    "while time_step <= max_training_timesteps:\n",
    "    state, _ = env.reset()\n",
    "    current_ep_reward = 0\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "\n",
    "        if time_step % ppo_agent.update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        if time_step % print_freq == 0:\n",
    "            display.clear_output()\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "            print(f\"Episode: {i_episode} Timestep: {time_step} Reward: {print_avg_reward}\")\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "            plt.plot(list_episodes_reward)\n",
    "            plt.show()\n",
    "\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print('------------------------')\n",
    "            ppo_agent.save(f\"ppo_actor.pt\")\n",
    "            print('model saved')\n",
    "            print('------------------------')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    list_episodes_reward.append(current_ep_reward)\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
