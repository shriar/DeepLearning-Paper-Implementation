{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces.box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from IPython import display\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_frame42(frame):\n",
    "    frame = frame[34:34 + 160, :160]\n",
    "    # Resize by half, then down to 42x42 (essentially mipmapping). If\n",
    "    # we resize directly we lose pixels that, when mapped to 42x42,\n",
    "    # aren't close enough to the pixel boundary.\n",
    "    frame = cv2.resize(frame, (80, 80))\n",
    "    frame = cv2.resize(frame, (42, 42))\n",
    "    frame = frame.mean(2, keepdims=True)\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame *= (1.0 / 255.0)\n",
    "    frame = np.moveaxis(frame, -1, 0)\n",
    "    return frame\n",
    "\n",
    "class AtariRescale42x42(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(AtariRescale42x42, self).__init__(env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 42, 42])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return _process_frame42(observation)\n",
    "    \n",
    "class NormalizedEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(NormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.num_steps += 1\n",
    "        self.state_mean = self.state_mean * self.alpha + \\\n",
    "            observation.mean() * (1 - self.alpha)\n",
    "        self.state_std = self.state_std * self.alpha + \\\n",
    "            observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "\n",
    "max_episode_steps = 1000000\n",
    "def create_atari_env(env_id):\n",
    "    env = gym.make(env_id, max_episode_steps=max_episode_steps)\n",
    "    # env = gym.make(env_id, max_episode_steps=max_episode_steps, render_mode=\"human\")\n",
    "    env = AtariRescale42x42(env)\n",
    "    env = NormalizedEnv(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = create_atari_env(\"ALE/Pong-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 42, 42), 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.n\n",
    "state_dim, action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(torch.nn.Module):\n",
    "    def __init__(self, num_inputs=state_dim[0], action_dim=action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "\n",
    "        # self.lstm = nn.LSTMCell(32 * 3 * 3, 256)\n",
    "        self.fc = nn.Linear(32 * 3 * 3, 256)\n",
    "\n",
    "        num_outputs = action_dim\n",
    "        self.critic_linear = nn.Linear(256, 1)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "        self.soft = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state = torch.from_numpy(state)\n",
    "        x = F.elu(self.conv1(state))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "\n",
    "        x = x.view(-1, 32 * 3 * 3)\n",
    "        x = self.fc(x)\n",
    "        action_probs = self.soft(self.actor_linear(x))\n",
    "        state_val = self.critic_linear(x)\n",
    "        return action_probs, state_val\n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs, state_val = self(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):  \n",
    "        action_probs, state_val = self(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, state_val, dist_entropy\n",
    "\n",
    "\n",
    "state, _ = env.reset()\n",
    "# state = torch.from_numpy(state)\n",
    "# model = ActorCritic()\n",
    "# cx = torch.zeros(1, 256).float()\n",
    "# hx = torch.zeros(1, 256).float()\n",
    "# # action_probs, state_val, (hx, cx) = model((state, (hx, cx)))\n",
    "# # action_probs.shape, state_val.shape\n",
    "# action, action_logprob, state_val = model.act(state)\n",
    "# action_logprobs, state_val, dist_entropy = model.evaluate(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 42, 42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim=state_dim[0], action_dim=action_dim, lr_actor=0.0003 , lr_critic=0.001 , gamma=0.99 , K_epochs=80, eps_clip=0.2):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "        self.time_buffer_clear = 0\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_actor)\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "        def create_discount_matrix(gamma, n):\n",
    "            temp = torch.tensor([gamma ** i for i in range(n)], dtype=torch.float32)\n",
    "            final = torch.zeros((n, n), dtype=torch.float32)\n",
    "            for i in range(n):\n",
    "                final[i:, i] = temp[:n-i]\n",
    "\n",
    "            return final\n",
    "        \n",
    "        self.discount_matrix = create_discount_matrix(gamma, 10000).to(device)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            action, action_logprob, state_val = self.policy_old.act(state)\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        rewards = torch.tensor(self.buffer.rewards).view(1, -1).float().to(device)\n",
    "        rewards = rewards @ self.discount_matrix\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "        rewards = rewards.view(-1)\n",
    "\n",
    "        old_states = torch.stack(self.buffer.states, dim=0).detach().to(device)\n",
    "        # old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
    "\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.buffer.clear()\n",
    "        # if self.time_buffer_clear == 0:\n",
    "        #     self.time_buffer_clear = 0\n",
    "        #     self.buffer.clear()\n",
    "        # self.time_buffer_clear  += 1\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "  \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        \n",
    "ppo_agent = PPO()\n",
    "ppo_agent.load(f\"ppo_actor_kaggle.pt\")\n",
    "# list_episodes_reward = []\n",
    "time_step = 0\n",
    "# i_episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "-1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# ppo_agent.load(f\"ppo_actor.pt\")\n",
    "def create_atari_env(env_id):\n",
    "    # env = gym.make(env_id, max_episode_steps=max_episode_steps)\n",
    "    env = gym.make(env_id, max_episode_steps=max_episode_steps, render_mode=\"human\")\n",
    "    env = AtariRescale42x42(env)\n",
    "    env = NormalizedEnv(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = create_atari_env(\"ALE/Pong-v5\")\n",
    "state, info = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "while True:\n",
    "    # action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    action = ppo_agent.select_action(state)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if reward != 0:\n",
    "        print(reward)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        print(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# from gymnasium.spaces import Box\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# def _process_frame42(frame):\n",
    "#     frame = frame[34:34 + 160, :160]\n",
    "#     frame = cv2.resize(frame, (80, 80))\n",
    "#     frame = cv2.resize(frame, (42, 42))\n",
    "#     frame = frame.mean(2, keepdims=True)\n",
    "#     frame = frame.astype(np.float32)\n",
    "#     frame *= (1.0 / 255.0)\n",
    "#     frame = np.moveaxis(frame, -1, 0)\n",
    "#     return frame\n",
    "\n",
    "# class AtariRescale42x42(gym.ObservationWrapper):\n",
    "#     def __init__(self, env=None):\n",
    "#         super(AtariRescale42x42, self).__init__(env)\n",
    "#         self.observation_space = Box(0.0, 1.0, [1, 42, 42])\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         return _process_frame42(observation)\n",
    "\n",
    "# class NormalizedEnv(gym.ObservationWrapper):\n",
    "#     def __init__(self, env=None):\n",
    "#         super(NormalizedEnv, self).__init__(env)\n",
    "#         self.state_mean = 0\n",
    "#         self.state_std = 0\n",
    "#         self.alpha = 0.9999\n",
    "#         self.num_steps = 0\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         self.num_steps += 1\n",
    "#         self.state_mean = self.state_mean * self.alpha + \\\n",
    "#             observation.mean() * (1 - self.alpha)\n",
    "#         self.state_std = self.state_std * self.alpha + \\\n",
    "#             observation.std() * (1 - self.alpha)\n",
    "\n",
    "#         unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "#         unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "#         return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "\n",
    "# class CustomPongEnv(gym.Wrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super(CustomPongEnv, self).__init__(env)\n",
    "#         self.last_hit_by_player = False\n",
    "#         self.ball_x_previous = None\n",
    "\n",
    "#     def reset(self, **kwargs):\n",
    "#         self.last_hit_by_player = False\n",
    "#         self.ball_x_previous = None\n",
    "#         return super().reset(**kwargs)\n",
    "\n",
    "#     def get_ball_position(self, observation):\n",
    "#         # The observation is now a 1x42x42 array\n",
    "#         # We'll use the middle row to estimate the ball's x position\n",
    "#         middle_row = observation[0, 21, :]\n",
    "#         ball_x = np.argmax(middle_row)\n",
    "#         return ball_x if middle_row[ball_x] > 0 else None\n",
    "\n",
    "#     def step(self, action):\n",
    "#         observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "#         ball_x = self.get_ball_position(observation)\n",
    "        \n",
    "#         if ball_x is not None and self.ball_x_previous is not None:\n",
    "#             if ball_x < self.ball_x_previous and self.last_hit_by_player:\n",
    "#                 # Ball is moving left after player hit, opponent hit the ball\n",
    "#                 reward = -1\n",
    "#                 self.last_hit_by_player = False\n",
    "#             elif ball_x > self.ball_x_previous and not self.last_hit_by_player:\n",
    "#                 # Ball is moving right after opponent hit, player hit the ball\n",
    "#                 reward = 1\n",
    "#                 self.last_hit_by_player = True\n",
    "        \n",
    "#         self.ball_x_previous = ball_x\n",
    "        \n",
    "#         return observation, reward, terminated, truncated, info\n",
    "\n",
    "# def create_custom_pong_env(env_id, max_episode_steps=1000000, render_mode='human'):\n",
    "#     env = gym.make(env_id, max_episode_steps=max_episode_steps, render_mode=render_mode)\n",
    "#     env = AtariRescale42x42(env)\n",
    "#     env = NormalizedEnv(env)\n",
    "#     env = CustomPongEnv(env)\n",
    "#     return env\n",
    "\n",
    "# # Usage\n",
    "# env = create_custom_pong_env(\"ALE/Pong-v5\")\n",
    "# state, info = env.reset()\n",
    "\n",
    "# for _ in range(10000):  # Run for more steps to see the effects\n",
    "#     # action = env.action_space.sample()  # Replace with your agent's action\n",
    "#     action = ppo_agent.select_action(state)\n",
    "#     state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "#     if reward != 0:\n",
    "#         print(f\"Reward: {reward}\")\n",
    "    \n",
    "#     if terminated or truncated:\n",
    "#         state, info = env.reset()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
