{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python train_rmc.py --cuda --adaptivesoftmax --cutoffs 1000 5000 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn, TinyJit\n",
    "from typing import List, Callable\n",
    "import pickle\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 192\n",
    "num_heads = 4\n",
    "key_size = 64\n",
    "mem_slots = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 100//2\n",
    "batch_size = 64//2\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data[:nbatch * bsz]\n",
    "    data = data.view(bsz, -1).T.contiguous()\n",
    "    return data\n",
    "\n",
    "loadfile = open('relational-rnn-pytorch\\data\\corpus-wikitext-2.pkl', 'rb')\n",
    "corpus = pickle.load(loadfile)\n",
    "\n",
    "data_T = Tensor(corpus.train.numpy())\n",
    "train_data = batchify(data_T, batch_size)\n",
    "ntokens = len(corpus.dictionary)\n",
    "# train_data = batchify(corpus.train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory:\n",
    "    def __init__(self, mem_slots=1, head_size=192//2, input_size=192//2, num_tokens=ntokens, num_heads=4, num_blocks=1, attention_mlp_layers=3, key_size=64//2):\n",
    "        ########## generic parameters for RMC ##########\n",
    "        self.head_size = head_size\n",
    "        self.num_heads = num_heads\n",
    "        self.mem_size = self.head_size * self.num_heads\n",
    "        self.mem_slots = mem_slots\n",
    "\n",
    "        self.mem_slots_plus_input = self.mem_slots + 1\n",
    "        self.num_blocks = num_blocks\n",
    "        self.attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        ########## parameters for multihead attention ##########\n",
    "        self.key_size = key_size if key_size else self.head_size\n",
    "        self.value_size = self.head_size\n",
    "        self.qkv_size = 2 * self.key_size + self.value_size\n",
    "        self.total_qkv_size = self.qkv_size * self.num_heads  # denoted as F\n",
    "\n",
    "\n",
    "        self.qkv_projector = nn.Linear(self.mem_size, self.total_qkv_size)\n",
    "        self.qkv_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.total_qkv_size])\n",
    "\n",
    "        self.attention_mlp: List[Callable[[Tensor], Tensor]] = [*[nn.Linear(self.mem_size, self.mem_size), Tensor.relu]*self.attention_mlp_layers]\n",
    "        self.attended_memory_layernorm = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "        self.attended_memory_layernorm2 = nn.LayerNorm([self.mem_slots_plus_input, self.mem_size])\n",
    "\n",
    "        ########## parameters for initial embedded input projection ##########\n",
    "        self.input_size = input_size\n",
    "        self.input_projector = nn.Linear(self.input_size, self.mem_size)\n",
    "\n",
    "        ########## parameters for gating ##########\n",
    "        self.input_gate_projector = nn.Linear(self.mem_size, self.mem_size * 2)\n",
    "        self.memory_gate_projector = nn.Linear(self.mem_size, self.mem_size * 2)\n",
    "\n",
    "\n",
    "\n",
    "        ########## parameters for token-to-embed & output-to-token logit for softmax\n",
    "        self.dropout = Tensor.dropout\n",
    "        self.num_tokens = num_tokens # number of unique tokens\n",
    "        self.token_to_input_encoder = nn.Embedding(self.num_tokens, self.input_size)\n",
    "        self.output_to_embed_decoder = nn.Linear(self.mem_slots * self.mem_size, self.input_size)\n",
    "        \n",
    "        self.embed_to_logit_decoder = nn.Linear(self.input_size, self.num_tokens)\n",
    "        self.embed_to_logit_decoder.weight = self.token_to_input_encoder.weight\n",
    "\n",
    "        # self.criterion = Tensor.los\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        if isinstance(h, Tensor):\n",
    "            return h.detach()\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "        \n",
    "    def initial_state(self, batch_size):\n",
    "        init_state = Tensor.stack(*[Tensor.eye(self.mem_slots) for _ in range(batch_size)]) # (64, 1, 1)\n",
    "        difference = self.mem_size - self.mem_slots\n",
    "        pad = Tensor.zeros((batch_size, self.mem_slots, difference))  # (64, 1, 767)\n",
    "        init_state = Tensor.cat(*[init_state, pad], dim=2)\n",
    "        return init_state\n",
    "\n",
    "\n",
    "    def multihead_attention(self, memory):\n",
    "        qkv = self.qkv_projector(memory)\n",
    "        qkv = self.qkv_layernorm(qkv)\n",
    "\n",
    "        mem_slots = memory.shape[1]  # denoted as N\n",
    "\n",
    "        qkv_reshape = qkv.view(qkv.shape[0], mem_slots, self.num_heads, self.qkv_size)\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "        q, k, v = Tensor.split(qkv_transpose, [self.key_size, self.key_size, self.value_size], -1)\n",
    "\n",
    "        q = q * (self.key_size ** -0.5)\n",
    "\n",
    "        dot_product = q @ k.permute(0, 1, 3, 2)\n",
    "        weights = Tensor.softmax(dot_product, axis=-1)\n",
    "\n",
    "        output = weights @ v\n",
    "        output_transpose = output.permute(0, 2, 1, 3).contiguous()\n",
    "        new_memory = output_transpose.view(output_transpose.shape[0], output_transpose.shape[1], -1)\n",
    "        return new_memory\n",
    "    \n",
    "    def create_gates(self, inputs, memory):\n",
    "        memory = Tensor.tanh(memory)\n",
    "        \n",
    "        inputs = inputs.view(inputs.shape[0], -1)\n",
    "        gate_inputs = self.input_gate_projector(inputs)\n",
    "        gate_inputs = gate_inputs.unsqueeze(dim=1)\n",
    "        gate_memory = self.memory_gate_projector(memory)\n",
    "\n",
    "        gates = gate_memory + gate_inputs\n",
    "        gates = Tensor.split(gates, sizes=int(gates.shape[2] / 2), dim=2)\n",
    "        input_gate, forget_gate = gates\n",
    "\n",
    "        input_gate = Tensor.sigmoid(input_gate)\n",
    "        forget_gate = Tensor.sigmoid(forget_gate)\n",
    "\n",
    "        return input_gate, forget_gate\n",
    "\n",
    "\n",
    "    def attend_over_memory(self, memory):\n",
    "        for _ in range(self.num_blocks):\n",
    "            attended_memory = self.multihead_attention(memory)\n",
    "            memory = self.attended_memory_layernorm(memory + attended_memory)\n",
    "            attention_mlp_T = memory\n",
    "\n",
    "            attention_mlp_T = attention_mlp_T.sequential(self.attention_mlp)\n",
    "            memory = self.attended_memory_layernorm2(memory + attention_mlp_T)\n",
    "\n",
    "        return memory\n",
    "    \n",
    "    def forward_step(self, inputs, memory):\n",
    "        inputs = inputs.view(-1, 1)\n",
    "        inputs_embed = self.dropout(self.token_to_input_encoder(inputs))\n",
    "        inputs_embed = inputs_embed.view(inputs_embed.shape[0], -1)\n",
    "        inputs_embed = self.input_projector(inputs_embed)\n",
    "\n",
    "        inputs_reshape = inputs_embed.unsqueeze(dim=1)\n",
    "        memory_plus_input = Tensor.cat(*[memory, inputs_reshape], dim=1)\n",
    "        next_memory = self.attend_over_memory(memory_plus_input)\n",
    "        \n",
    "        n = inputs_reshape.shape[1]\n",
    "        next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "        input_gate, forget_gate = self.create_gates(inputs_reshape, memory)\n",
    "\n",
    "        next_memory = input_gate * Tensor.tanh(next_memory) + forget_gate * memory\n",
    "\n",
    "        output = next_memory.view(next_memory.shape[0], -1)\n",
    "        output_embed = self.output_to_embed_decoder(output)\n",
    "        output_embed = self.dropout(output_embed)\n",
    "\n",
    "        logit = self.embed_to_logit_decoder(output_embed)\n",
    "        return logit, next_memory\n",
    "    \n",
    "    def __call__(self, inputs, memory, targets):\n",
    "        memory = self.repackage_hidden(memory)\n",
    "        \n",
    "        logits = []        \n",
    "\n",
    "        for idx_step in range(inputs.shape[1]):\n",
    "            logit, memory = self.forward_step(inputs[:, idx_step], memory)\n",
    "            logits.append(logit)\n",
    "\n",
    "        logits = Tensor.cat(*logits)\n",
    "        loss =logits.sparse_categorical_crossentropy(targets)\n",
    "        return loss, memory\n",
    "\n",
    "# seq_length = 2\n",
    "# input_size_ = 15\n",
    "\n",
    "model = RelationalMemory()\n",
    "# memory = Tensor.rand(64, mem_slots, head_size*num_heads)\n",
    "# inputs = Tensor.randint((64, 100))\n",
    "# target = Tensor.randint((64*100/2, 1))\n",
    "# model.attend_over_memory(memory).shape\n",
    "# model.forward_step(inputs, memory)\n",
    "# inputs = Tensor.rand((2, 1, 768))\n",
    "# model.create_gates(inputs, memory)\n",
    "# model.repackage_hidden(memory)\n",
    "# model(inputs, memory, target)\n",
    "# model.initial_state(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nn.optim.Adam(nn.state.get_parameters(model), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i + 1:i + 1 + seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TinyJit\n",
    "def train_step(train_data, memory, i) -> Tensor:\n",
    "    with Tensor.train():\n",
    "        optimizer.zero_grad()\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        data = data.T\n",
    "        targets = targets.view(-1, 1)\n",
    "        loss, memory = model(data, memory, targets)\n",
    "        loss = loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = model.initial_state(64//2)  # torch.Size([64, 1, 768])\n",
    "total_loss = 0.\n",
    "\n",
    "for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "    with Tensor.train():\n",
    "        loss, memory = train_step(train_data, memory, i)\n",
    "        print(loss.item())\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
