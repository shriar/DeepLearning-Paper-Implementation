{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from random import shuffle\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from pickle import Pickler, Unpickler\n",
    "\n",
    "from othello.utils import *\n",
    "from othello.OthelloGame import OthelloGame\n",
    "log = logging.getLogger(__name__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = OthelloGame(6)\n",
    "# board = game.getInitBoard()\n",
    "# game.stringRepresentation(board)\n",
    "# board.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "})\n",
    "\n",
    "class OthelloNNet(nn.Module):\n",
    "    def __init__(self, game, args):\n",
    "        # game params\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.args = args\n",
    "\n",
    "        super(OthelloNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "        self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
    "        self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
    "\n",
    "        self.fc1 = nn.Linear(args.num_channels*(self.board_x-4)*(self.board_y-4), 1024)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, self.action_size)\n",
    "\n",
    "        self.fc4 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        #                                                           s: batch_size x board_x x board_y\n",
    "        s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "        s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "        s = s.view(-1, self.args.num_channels*(self.board_x-4)*(self.board_y-4))\n",
    "\n",
    "        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "        s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "        pi = self.fc3(s)                                                                         # batch_size x action_size\n",
    "        v = self.fc4(s)                                                                          # batch_size x 1\n",
    "\n",
    "        # return pi, torch.tanh(v)\n",
    "        return F.log_softmax(pi, dim=1), torch.tanh(v)\n",
    "    \n",
    "# nnet = OthelloNNet(game, args).eval()\n",
    "# action_logprob, value = nnet(torch.from_numpy(board).float())\n",
    "\n",
    "class NNetWrapper():\n",
    "    def __init__(self, game):\n",
    "        self.nnet = OthelloNNet(game, args).to(device)\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "        self.optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "    def train(self, examples):\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch + 1))\n",
    "            self.nnet.train()\n",
    "            # pi_losses = AverageMeter()\n",
    "            # v_losses = AverageMeter()\n",
    "\n",
    "            batch_count = int(len(examples) / args.batch_size)\n",
    "            # print(batch_count)\n",
    "            t = tqdm(range(batch_count), desc='Training Net')\n",
    "            for _ in t:\n",
    "                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                boards = torch.FloatTensor(np.array(boards).astype(np.float64)).contiguous().to(device)\n",
    "                target_pis = torch.FloatTensor(np.array(pis)).contiguous().to(device)\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64)).contiguous().to(device)             \n",
    "                # print(boards.shape)\n",
    "                # print(target_pis.shape)\n",
    "                # print(target_vs.shape)\n",
    "                out_pi, out_v = self.nnet(boards)\n",
    "                # print(out_pi.shape)\n",
    "                # print(out_v.squeeze().shape)\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                \n",
    "                # break\n",
    "            # break\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, board):\n",
    "        board = torch.FloatTensor(board.astype(np.float64)).contiguous().to(device)\n",
    "        board = board.view(1, 1, self.board_x, self.board_y)\n",
    "        self.nnet.eval()   # NOTE: why eval?\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.nnet(board)\n",
    "        \n",
    "        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        pass\n",
    "\n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        pass\n",
    "\n",
    "game = OthelloGame(6)\n",
    "board = game.getInitBoard()\n",
    "\n",
    "nnet = NNetWrapper(game)\n",
    "# nnet.predict(board)\n",
    "nnet.train(trainExamplesHistory[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('othello\\\\temp\\\\checkpoint_1.pth.tar.examples', \"rb\") as f:\n",
    "    trainExamplesHistory = Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "class MCTS():\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        \n",
    "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # stores #times edge s,a was visited\n",
    "        self.Ns = {}  # stores #times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "        # print(counts)\n",
    "\n",
    "        if temp == 0:\n",
    "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
    "            bestA = np.random.choice(bestAs)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            # print(bestAs, bestA)\n",
    "            return probs\n",
    "\n",
    "        counts = [count ** (1. / temp) for count in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x / counts_sum for x in counts]\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # terminal node\n",
    "            return -self.Es[s] # NOTE: why negative?\n",
    "        \n",
    "        if s not in self.Ps:\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "            # print(self.Ps[s])\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            # print(sum_Ps_s)\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            # print(self.Ps[s])\n",
    "            else:\n",
    "                log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            # print(valids)\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (1 + self.Nsa[(s, a)])\n",
    "                else:\n",
    "                    # print(self.Ns[s])\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "        # print(next_player)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "        \n",
    "        v = self.search(next_s)\n",
    "        \n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        # print(len(self.Ns), '\\n\\n')\n",
    "        # print(self.Nsa, '\\n\\n')\n",
    "\n",
    "        return -v\n",
    "\n",
    "\n",
    "game = OthelloGame(6)\n",
    "board = game.getInitBoard()\n",
    "canonicalBoard = game.getCanonicalForm(board, 1)\n",
    "\n",
    "mcts = MCTS(game, nnet, args)\n",
    "# mcts.search(canonicalBoard)\n",
    "# mcts.getActionProb(canonicalBoard, temp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 1000,\n",
    "    'numEps': 100,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,        #\n",
    "    'updateThreshold': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200000,    # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 25,          # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 40,         # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "\n",
    "    'checkpoint': 'othello/temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "})\n",
    "\n",
    "class Coach():\n",
    "    def __init__(self, game, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        # self.pnet = self.nnet.__class__(self.game)  # the competitor network\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.nnet, self.args)\n",
    "        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations\n",
    "        self.skipFirstSelfPlay = False  # can be overriden in loadTrainExamples()\n",
    "\n",
    "    def executeEpisode(self):\n",
    "        trainExamples = []\n",
    "        board = self.game.getInitBoard()\n",
    "        self.curPlayer = 1\n",
    "        episodeStep = 0\n",
    "\n",
    "        while True:\n",
    "            episodeStep += 1\n",
    "            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)\n",
    "            temp = int(episodeStep < self.args.tempThreshold)\n",
    "\n",
    "            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)\n",
    "            # print(pi)\n",
    "            # print(len(pi))\n",
    "            sym = self.game.getSymmetries(canonicalBoard, pi)\n",
    "            # print(sym[2][0])\n",
    "            for b, p in sym:\n",
    "                trainExamples.append([b, self.curPlayer, p, None])\n",
    "            \n",
    "            action = np.random.choice(len(pi), p=pi)\n",
    "            # print(action)\n",
    "            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n",
    "            r = self.game.getGameEnded(board, self.curPlayer)\n",
    "\n",
    "            # print(r)            \n",
    "            if r != 0:\n",
    "                # a = [(r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n",
    "                # b = [((-1) ** (x[1] != self.curPlayer)) for x in trainExamples]\n",
    "                # print(a, '\\n')\n",
    "                # print(b, '\\n')\n",
    "                return [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n",
    "                # break\n",
    "            \n",
    "            # break\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(1, self.args.numIters + 1):\n",
    "            print(f'Starting Iter #{i} ...')\n",
    "            if not self.skipFirstSelfPlay or i > 1:\n",
    "                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n",
    "\n",
    "                for _ in tqdm(range(self.args.numEps), desc=\"Self Play\"):\n",
    "                    self.mcts = MCTS(self.game, self.nnet, self.args)  # reset search tree\n",
    "                    iterationTrainExamples += self.executeEpisode()\n",
    "\n",
    "                self.trainExamplesHistory.append(iterationTrainExamples)\n",
    "\n",
    "            # if len(self.trainExamplesHistory)             for e in self.trainExamplesHistory:\n",
    "\n",
    "\n",
    "            trainExamples = []\n",
    "            for e in self.trainExamplesHistory:\n",
    "                trainExamples.extend(e)\n",
    "            shuffle(trainExamples)\n",
    "\n",
    "            self.nnet.train(trainExamples)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getCheckpointFile(self, iteration):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def saveTrainExamples(self, iteration):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def loadTrainExamples(self):\n",
    "        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n",
    "        examplesFile = modelFile + \".examples\"\n",
    "        if not os.path.isfile(examplesFile):\n",
    "            log.warning(f'File \"{examplesFile}\" with trainExamples not found!')\n",
    "            r = input(\"Continue? [y|n]\")\n",
    "            if r != \"y\":\n",
    "                sys.exit()\n",
    "        else:\n",
    "            log.info(\"File with trainExamples found. Loading it...\")\n",
    "            with open(examplesFile, \"rb\") as f:\n",
    "                self.trainExamplesHistory = Unpickler(f).load()\n",
    "            log.info('Loading done!')\n",
    "\n",
    "            # examples based on the model were already collected (loaded)\n",
    "            self.skipFirstSelfPlay = True\n",
    "\n",
    "\n",
    "    \n",
    "c = Coach(game, args)\n",
    "# c.executeEpisode()[0][2]\n",
    "# c.learn()\n",
    "c.loadTrainExamples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
